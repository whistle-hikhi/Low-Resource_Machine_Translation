{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpWvQb1kAdYDeB1Fec0mGg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained mBART50"
      ],
      "metadata": {
        "id": "aVG-9zM6o60H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Dataset"
      ],
      "metadata": {
        "id": "iKoNiGEOk4b5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C6F4s2Uykq4x"
      },
      "outputs": [],
      "source": [
        " # install libs\n",
        "!pip install -q transformers sentencepiece datasets accelerate evaluate sacrebleu\n",
        "# import libs\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "        MBart50TokenizerFast,\n",
        "        AutoModelForSeq2SeqLM,\n",
        "        DataCollatorForSeq2Seq,\n",
        "        Seq2SeqTrainingArguments,\n",
        "        Seq2SeqTrainer)\n",
        "\n",
        "# build dataset\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, cfg, data_type=\"train\"):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.src_texts, self.tgt_texts = self.read_data(data_type)\n",
        "        self.src_input_ids = self.texts_to_sequences(self.src_texts)\n",
        "        self.labels = self.texts_to_sequences(self.tgt_texts)\n",
        "\n",
        "    def read_data(self, data_type):\n",
        "        data = load_dataset(\n",
        "        \"mt_eng_vietnamese\",\n",
        "        \"iwslt2015-en-vi\",\n",
        "        split=data_type\n",
        "        )\n",
        "        src_texts = [sample[\"translation\"][self.cfg.src_lang] for sample in data]\n",
        "        tgt_texts = [sample[\"translation\"][self.cfg.tgt_lang] for sample in data]\n",
        "        return src_texts, tgt_texts\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        data_inputs = self.cfg.tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=self.cfg.max_len,\n",
        "        return_tensors='pt'\n",
        "        )\n",
        "        return data_inputs.input_ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "        \"input_ids\": self.src_input_ids[idx],\n",
        "        \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_input_ids)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer, Model, Metric"
      ],
      "metadata": {
        "id": "XCEhYMVJl-gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseConfig:\n",
        "    \"\"\" base Encoder Decoder config \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class NMTConfig(BaseConfig):\n",
        "    # Data\n",
        "    src_lang = 'en'\n",
        "    tgt_lang = 'vi'\n",
        "    max_len = 75\n",
        "    add_special_tokens = True\n",
        "\n",
        "    # Model\n",
        "    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "\n",
        "    # Training\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    learning_rate = 5e-5\n",
        "    train_batch_size = 32\n",
        "    eval_batch_size = 32\n",
        "    num_train_epochs = 2\n",
        "    save_total_limit = 1\n",
        "    ckpt_dir = f'./mbart50-{src_lang}-{tgt_lang}'\n",
        "    eval_steps = 1000\n",
        "\n",
        "    # Inference\n",
        "    beam_size = 5\n",
        "\n",
        "cfg = NMTConfig()\n",
        "cfg.tokenizer = MBart50TokenizerFast.from_pretrained(cfg.model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(cfg.model_name)\n",
        "\n",
        "# metric\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    preds= np.where(preds !=-100, preds, cfg.tokenizer.pad_token_id)\n",
        "    decoded_preds = cfg.tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    labels= np.where(labels !=-100, labels, cfg.tokenizer.pad_token_id)\n",
        "    decoded_labels = cfg.tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != cfg.tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-g86U22l9ov",
        "outputId": "01943f3b-f864-448f-9b73-07da649f1fc1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer"
      ],
      "metadata": {
        "id": "4k85NhWFmZN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = NMTDataset(cfg, data_type=\"train\")\n",
        "valid_dataset = NMTDataset(cfg, data_type=\"validation\")\n",
        "test_dataset = NMTDataset(cfg, data_type=\"test\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy='steps',\n",
        "    save_steps=cfg.eval_steps,\n",
        "    eval_steps=cfg.eval_steps,\n",
        "    output_dir=cfg.ckpt_dir,\n",
        "    per_device_train_batch_size=cfg.train_batch_size,\n",
        "    per_device_eval_batch_size=cfg.eval_batch_size,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    save_total_limit=cfg.save_total_limit,\n",
        "    num_train_epochs=cfg.num_train_epochs,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    cfg.tokenizer,\n",
        "    model=model\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=cfg.tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "KyuCBepdmajz",
        "outputId": "3ecfaa87-9c86-447f-9f76-9b5fdd63544a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='8334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   2/8334 : < :, Epoch 0.00/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.47 GiB is free. Process 31400 has 13.28 GiB memory in use. Of the allocated memory 10.74 GiB is allocated by PyTorch, and 2.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2f65e0810c2d>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1539\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1869\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1871\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2771\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2772\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2794\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2795\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2796\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         )\n\u001b[0;32m-> 1581\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.24 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1.47 GiB is free. Process 31400 has 13.28 GiB memory in use. Of the allocated memory 10.74 GiB is allocated by PyTorch, and 2.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Qz9EP_OnmgPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(\n",
        "    text,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    device=\"cpu\",\n",
        "    max_length=75,\n",
        "    beam_size=5\n",
        "    ):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "    input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_length=max_length,\n",
        "    early_stopping=True,\n",
        "    num_beams=beam_size,\n",
        "    length_penalty=2.0\n",
        "    )\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    return output_str\n",
        " sentence = 'i go to school'\n",
        " inference(sentence, cfg.tokenizer, model)"
      ],
      "metadata": {
        "id": "h-1BtW0xmheI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back-Translation"
      ],
      "metadata": {
        "id": "ruRjBHXRpCzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Vi-En Model"
      ],
      "metadata": {
        "id": "rSxNpQZSqO4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Dataset"
      ],
      "metadata": {
        "id": "vU1DWGVBpYXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build dataset\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, cfg, data_type='train'):\n",
        "        super().__init__()\n",
        "        self.cfg\n",
        "        self.src_texts, self.tgt_texts = self.read_data(data_type)\n",
        "        self.src_input_ids = self.texts_to_sequences(self.src_texts)\n",
        "        self.labels = self.texts_to_sequences(self.tgt_texts)\n",
        "\n",
        "    def read_data(self, data_type):\n",
        "        data = load_dataset(\n",
        "        \"mt_eng_vietnamese\",\n",
        "        \"iwslt2015-en-vi\",\n",
        "        split=data_type\n",
        "        )\n",
        "        src_texts = [sample[\"translation\"][self.cfg.src_lang] for sample in data]\n",
        "        tgt_texts = [sample[\"translation\"][self.cfg.tgt_lang] for sample in data]\n",
        "        return src_texts, tgt_texts\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        data_inputs = self.cfg.tokenizer(\n",
        "            texts,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.cfg.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return data_inputs.input_ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "        \"input_ids\": self.src_input_ids[idx],\n",
        "        \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_input_ids)[0]"
      ],
      "metadata": {
        "id": "qbES4Nn9pTol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer, Model, Metric"
      ],
      "metadata": {
        "id": "bcufpJj8p0r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseConfig:\n",
        "    \"\"\" base Encoder Decoder config \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class NMTConfig(BaseConfig):\n",
        "    # Data\n",
        "    src_lang = 'vi'\n",
        "    tgt_lang = 'en'\n",
        "    max_len = 75\n",
        "    add_special_tokens = True\n",
        "\n",
        "    # Model\n",
        "    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "\n",
        "    # Training\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    learning_rate = 5e-5\n",
        "    train_batch_size = 32\n",
        "    eval_batch_size = 32\n",
        "    num_train_epochs = 2\n",
        "    save_total_limit = 1\n",
        "    ckpt_dir = f'./mbart50-{src_lang}-{tgt_lang}'\n",
        "    eval_steps = 1000\n",
        "\n",
        "    # Inference\n",
        "    beam_size = 5\n",
        "\n",
        "cfg = NMTConfig()\n",
        "cfg.tokenizer = MBart50TokenizerFast.from_pretrained(cfg.model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(cfg.model_name)\n",
        "\n",
        " # metric\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "      preds = preds[0]\n",
        "\n",
        "    preds= np.where(preds !=-100, preds, cfg.tokenizer.pad_token_id)\n",
        "    decoded_preds = cfg.tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    labels= np.where(labels !=-100, labels, cfg.tokenizer.pad_token_id)\n",
        "    decoded_labels = cfg.tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != cfg.tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "o1OeKCqBp2nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "Xk_ARzB1p7_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #training\n",
        "train_dataset = NMTDataset(cfg, data_type=\"train\")\n",
        "valid_dataset = NMTDataset(cfg, data_type=\"validation\")\n",
        "test_dataset = NMTDataset(cfg, data_type=\"test\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy='steps',\n",
        "    save_steps=cfg.eval_steps,\n",
        "    eval_steps=cfg.eval_steps,\n",
        "    output_dir=cfg.ckpt_dir,\n",
        "    per_device_train_batch_size=cfg.train_batch_size,\n",
        "    per_device_eval_batch_size=cfg.eval_batch_size,\n",
        "      learning_rate=cfg.learning_rate,\n",
        "    save_total_limit=cfg.save_total_limit,\n",
        "    num_train_epochs=cfg.num_train_epochs,\n",
        "    load_best_model_at_end=True,\n",
        "    )\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    cfg.tokenizer,\n",
        "    model=model\n",
        "    )\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=cfg.tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        "    )\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fzvB1I9tp814"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Synthetic Data Generation"
      ],
      "metadata": {
        "id": "iUC578_7qCPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load vi data from phomt dataset\n",
        "with open('./test.vi', 'r') as f:\n",
        "    vi_phomt = f.readlines()\n",
        "# inference a sentence or bacth\n",
        "def inference(\n",
        "    text,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    device=\"cpu\",\n",
        "    max_length=75,\n",
        "    beam_size=5\n",
        "    ):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "        )\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.attention_mask.to(device)\n",
        "    model.to(device)\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        early_stopping=True,\n",
        "        num_beams=beam_size,\n",
        "        length_penalty=2.0\n",
        "    )\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return output_str[0]\n",
        "\n",
        "en_phomt_preds = []\n",
        "for sent in vi_phomt:\n",
        "    en_sent = inference(sent, cfg.tokenizer, model)\n",
        "    en_phomt_preds.append(en_sent)"
      ],
      "metadata": {
        "id": "BR7nm9nfqEQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train En-Vi model"
      ],
      "metadata": {
        "id": "0g1BUE8pqZXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Dataset"
      ],
      "metadata": {
        "id": "-7tTyRYvqeRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # build dataset\n",
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, cfg, src_augs=[], tgt_augs=[], data_type=\"train\"):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.src_texts, self.tgt_texts = self.read_data(data_type)\n",
        "        if data_type == 'train':\n",
        "        self.src_texts.extend(src_augs)\n",
        "        self.tgt_texts.extend(tgt_augs)\n",
        "\n",
        "        self.src_input_ids = self.texts_to_sequences(self.src_texts)\n",
        "        self.labels = self.texts_to_sequences(self.tgt_texts)\n",
        "\n",
        "    def read_data(self, data_type):\n",
        "        data = load_dataset(\n",
        "        \"mt_eng_vietnamese\",\n",
        "        \"iwslt2015-en-vi\",\n",
        "        split=data_type\n",
        "        )\n",
        "        src_texts = [sample[\"translation\"][self.cfg.src_lang] for sample in data]\n",
        "        tgt_texts = [sample[\"translation\"][self.cfg.tgt_lang] for sample in data]\n",
        "        return src_texts, tgt_texts\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        data_inputs = self.cfg.tokenizer(\n",
        "            texts,\n",
        "            padding=’max_length’,\n",
        "            truncation=True,\n",
        "            max_length=self.cfg.max_len,\n",
        "            return_tensors=’pt’\n",
        "            )\n",
        "        return data_inputs.input_ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "        \"input_ids\": self.src_input_ids[idx],\n",
        "        \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_input_ids)[0]"
      ],
      "metadata": {
        "id": "v5q6ChdFqchW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer, Model, Metric"
      ],
      "metadata": {
        "id": "GuIripk0qg5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # config\n",
        "class BaseConfig:\n",
        "  \"\"\" base Encoder Decoder config \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class NMTConfig(BaseConfig):\n",
        "    # Data\n",
        "    src_lang = 'en'\n",
        "    tgt_lang = 'vi'\n",
        "    max_len = 75\n",
        "    add_special_tokens = True\n",
        "\n",
        "    # Model\n",
        "    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "\n",
        "    # Training\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    learning_rate = 5e-5\n",
        "    train_batch_size = 32\n",
        "    eval_batch_size = 32\n",
        "    num_train_epochs = 2\n",
        "    save_total_limit = 1\n",
        "    ckpt_dir = f'./mbart50-{src_lang}-{tgt_lang}-backtranslation-2'\n",
        "    eval_steps = 1000\n",
        "\n",
        "    # Inference\n",
        "    beam_size = 5\n",
        "\n",
        "cfg = NMTConfig()\n",
        "\n",
        "cfg.tokenizer = MBart50TokenizerFast.from_pretrained(cfg.model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(cfg.model_name)\n",
        "\n",
        "train_dataset = NMTDataset(cfg, en_phomt_preds, vi_phomt, data_type=\"train\")\n",
        "valid_dataset = NMTDataset(cfg, data_type=\"validation\")\n",
        "test_dataset = NMTDataset(cfg, data_type=\"test\")\n",
        " # metric\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "      preds = preds[0]\n",
        "\n",
        "    preds= np.where(preds !=-100, preds, cfg.tokenizer.pad_token_id)\n",
        "    decoded_preds = cfg.tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    labels= np.where(labels !=-100, labels, cfg.tokenizer.pad_token_id)\n",
        "    decoded_labels = cfg.tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    prediction_lens = [np.count_nonzero(pred != cfg.tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "TCDNNmNKqjh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "EMsp32aAqlZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #training\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy='steps',\n",
        "    save_steps=cfg.eval_steps,\n",
        "    eval_steps=cfg.eval_steps,\n",
        "    output_dir=cfg.ckpt_dir,\n",
        "    per_device_train_batch_size=cfg.train_batch_size,\n",
        "    per_device_eval_batch_size=cfg.eval_batch_size,\n",
        "    learning_rate=cfg.learning_rate,\n",
        "    save_total_limit=cfg.save_total_limit,\n",
        "    num_train_epochs=cfg.num_train_epochs,\n",
        "    load_best_model_at_end=True,\n",
        "    )\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    cfg.tokenizer,\n",
        "    model=model\n",
        "    )\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=cfg.tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        "    )\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "LRkXRTdYqm72"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}